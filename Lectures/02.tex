\lr{Статистика и вероятность. Случайные величины и распределения}
\subsection{Случайные величины, вероятность}
\bf Случайной величиной\н\index{Случайная величина} называется величина~$X$,
если все ее возможные значения образуют конечную или бесконечную
последовательность чисел~$x_1,\ldots$, $x_N$,
и если принятие ею каждого из этих значений есть случайное событие.

Несколько случайных событий называют\ж несовместными\н\index{Несовместные
события}, если появление одного из них исключает появление остальных событий в
том же испытании. Несколько событий образуют\ж полную группу\н, если в
результате испытания появится хотя бы одно из них.

\bf Вероятностью\н\index{Вероятность}
наступления данного случайного события~$x_k$ называется предел относительной
частоты
наступления данного события~$n_k/N$:
$$P(x_k)=\lim_{N\to\infty}\frac{n_k}{N}.$$
Из определения вероятности вытекают следующие ее свойства:
\begin{itemize}
\item вероятность достоверного события равна единице;
\item вероятность невозможного события равна нулю;
\item вероятность случайного события есть положительное число, заключенное
между единицей и нулем.
\end{itemize}

Термин\к вероятность\н справедлив относительно\ж дискретных\н случайных
величин\index{Случайная величина!дискретная}~--- величин, принимающих
отдельные, изолированные возможные значения с определенными вероятностями.
Число возможных значений дискретной случайной величины может быть конечным или
бесконечным.

Случайная величина может быть не только дискретной, но и\ж
непрерывной\н\index{Случайная величина!непрерывная}. В этом
случае она может принимать любое значение из заданной~--- конечной или
бесконечной~--- области действительных чисел.
Непрерывная случайная величина характеризуется\ж плотностью
вероятности\н\index{Плотность вероятности}:
$$\phi(x)=\lim_{\Delta x\to 0}\frac{P(x<X<x+\Delta x)}{\Delta
x}=\frac{dP}{dx}.$$
Вероятность попадания значений~$X$ в интервал~$(x_1,x_2)$ можно вычислить по
формуле:
$$P(x_1<X<x_2)=\Int_{x_1}^{x_2}\phi(x)\,dx.$$

\subsection{Характеристики случайных величин}
Две случайные величины~$X$ и~$Y$ называются\ж независимыми\н\index{Случайная
величина!независимая}, если наступление одного
из событий~$x_n$ никак не сказывается на результате события~$y_n$. В этом случае
вероятность совместного наступления событий~$x_n$ и~$y_n$ равна
$P(x_ny_n)=P(x_n)P(y_n)$.
Дальнейшие формулы в этом разделе подразумевают независимость входящих в них
случайных
величин, если зависимость не указана явно.

Набор случайных величин~$X$ можно охарактеризовать\ж средним
арифметическим\н\index{Среднее арифметическое}:
$\aver{X}=\frc1{N}\sum_{n=1}^N x_n$. Для малых~$N$ данная величина будет
отличаться
от\ж математического ожидания\н\index{Математическое ожидание}, определяемого по
формуле
$$M(X)\equiv\mean{X}=\lim_{N\to\infty}\frac1{N}\sum_{n=1}^N
x_n\quad\text{и}\quad
M(X)=\Infint x\phi(x)\,dx.$$
\it В дальнейшем для обозначения математического ожидания случайной
величины~$X$ будем использовать следующую запись: $\mean{X}$\н.
Таким образом, математическое ожидание является суммой произведений всех
возможных значений случайной величины на их вероятности (для непрерывных
случайных величин~--- интегральной суммой).

Математическое ожидание обладает следующими\к свойствами\н:
\begin{itemize}\setlength{\itemsep}{2pt}
\item $\mean\const=\const$;
\item $\mean{\sum\C_n\cdot X_n}=\sum\C_n\cdot \mean{X_n}$,
где $\C_n$~-- постоянная величина; 
\item $\mean{\prod X_n}=\prod \mean{X_n}$ (для независимых случайных величин);
\item $\mean{f(x)}=\Infint f(x)\phi(x)\,dx$ (для непрерывных случайных величин).
\end{itemize}

Вполне логично сравнить математическое ожидание случайной величины с ее средним
арифметическим, однако, насколько правомерной окажется замена
$\mean{X}\Leftrightarrow\aver{X}$?\ж Неравенство
Чебыш\"ева\н\index{Неравенство Чебыш\"ева} позволяет утверждать, что
вероятность того, что отклонение случайной величины~$X$ от ее математического
ожидания по абсолютной величине меньше положительного числа~$\epsilon$, не
меньше, чем $1-D(X)/\epsilon^2$:
$$P(|X-\mean{X}|<\epsilon)\ge 1-D(X)/\epsilon^2.$$
Из\ж теоремы Чебыш\"ева\н\index{Теорема!Чебыш\"ева} следует, что 
$$\lim_{n\to\infty} P\Bigl(\Bigl|\frac{\sum
X_n}{n}-\frac{\sum\mean{X_n}}{n}\Bigr|<\epsilon\Bigr)=1.$$
Таким образом,\к среднее арифметическое достаточно большого числа независимых
случайных величин утрачивает характер случайной величины\н.\ж Теорема
Бернулли\н\index{Теорема!Бернулли} утверждает, что если в каждом
из~$n$ независимых испытаний вероятность~$p$ появления события~$A$
постоянна, то отклонение относительной частоты от вероятности~$p$ по абсолютной
величине будет сколь угодно малым, если число испытаний достаточно велико:
$$\lim_{n\to\infty} P(|m/n-p|<\epsilon)=1.$$

Теоремы Чебыш\"ева и Бернулли являются частными случаями\ж закона больших
чисел\н\index{Закон!больших чисел}. Теорема Чебыш\"ева является наиболее общим
законом больших чисел, теорема Бернулли~--- простейшим. Упрощенно можно
сказать, что чем большей является количество испытаний~$n$, тем
ближе~$\aver{X}$ к~$\mean{X}$ и относительная частота $m/n$ к вероятности~$p$.

Помимо математического ожидания набор случайных величин характеризуется медианой
и модой.\ж Мода\н\index{Мода}~--- значение во множестве наблюдений, которое
встречается наиболее часто. Иногда мод может быть больше одной. В этом случае
можно сказать, что совокупность\к мультимодальна\н. Как правило
мультимодальность указывает на то, что набор данных не подчиняется нормальному
распределению.\ж Медиана\н\index{Медиана}~--- возможное значение
признака, которое делит отсортированную совокупность на две равные части:
50\%~<<нижних>> единиц ряда данных будут иметь значение признака не больше, чем
медиана, а 50\%~<<верхних>>~--- значения признака не меньше, чем медиана.

Если $f(x)=(x-x_0)^n$, говорят, что~$\mean{f(x)}$ является\ж
моментом\н\index{Момент} случайной величины порядка~$n$. При~$x_0=0$ момент
называют\ж начальным\н, а при~$x_0=\mean{X}$~---\ж центральным\н.
В частности, моменты нулевого порядка равны~1, начальный момент
первого порядка равен математическому ожиданию случайной величины; центральный
момент первого порядка равен нулю.

Центральный момент второго порядка называют\ж
дисперсией\н\index{Дисперсия}:
$$D(X)=\mean{(x-\mean{x})^2}.$$
Разброс случайной величины относительно математического ожидания
характеризуется\ж средним квадратическим
отклонением\н\index{Среднее квадратическое отклонение}~$\sigma=\sqrt{D}$.
Дисперсию можно вычислить и по упрощенному выражению:
$$D=\mean{(x-\mean{x})^2}=\mean{x^2}-\mean{2x\mean{x}}+\mean{\mean{x}^2}=
\mean{x^2}-\mean{x}^2\equiv M(x^2)-[M(x)]^2.$$

Дисперсия обладает следующими\к свойствами\н:
\begin{itemize}
\item $D(\const)=0$;
\item $D(\C X)=C^2 D(X)$, где $\C$~-- постоянная величина;
\item $D(\sum X_n)=\sum D(X_n)$.
\end{itemize}


\subsection{Законы распределения}
\bf Законом распределения\н\index{Закон!распределения}\к дискретной\н случайной
величины называют соответствие между возможными значениями и их вероятностями.
Его можно задать таблично, аналитически (в виде формулы) и графически.

Вероятность того, что значения случайной величины не превышают заданного
числа~$x$
называют\ж функцией распределения\н\index{Функция!распределения}:
$$F(x)\equiv P(X\le x)=\Int_{-\infty}^x\phi(x)\,dx.$$
Так как вероятность попадания значений~$(X)$ в промежуток~$(-\infty,\infty)$
равна
единице, плотность вероятности должна быть\ж нормирована\н\index{Нормировка}:
$$\Infint\phi(x)\,dx=1.$$
\it Вероятность того, что непрерывная случайная величина~$X$ примет одно
определенное значение равна нулю\н.

Исходя из смысла функции распределения, можно рассчитать вероятность того, что
случайная величина примет значение, заключенное в интервале~$[a,b]$:
$$P(a\le X\le b)=F(b)-F(a).$$

Наиболее известными законами распределения
являются равномерное, нормальное (гауссово), Пуассона, биномиальное,
экспоненциальное.

Говорят, что случайная величина имеет непрерывное\ж равномерное
распределение\н\index{Распределение!равномерное}
на
отрезке~$[a,b]$, где~$a$,~$b\in\mathbb{R}$, если ее плотность $\phi(x)$ имеет
вид
$$
\phi(x)=\begin{cases}\frac1{b-a}, & x\in [a,b] \\ 0, & x\not\in [a,b]
\end{cases}.
$$
Интегрируя, получим, для~$F(x)$:
$$F(x)= \begin{cases} 0, & x < a \\ {x-a \over b-a}, & a \le x < b \\ 1, & x \ge
b \end{cases}.
$$
Математическое ожидание и медиана для равномерного распределения совпадают с
серединой
отрезка~$[a,b]$, модой же является любое значение из этого отрезка.

\bf Биномиальным\н\index{Распределение!биномиальное} называют распределение
вероятностей, определяемое\ж формулой Бернулли\н\index{Формула Бернулли}
$$P_n(k)=C_n^k p^k q^{n-k},\quad C_n^k=\frac{n!}{k!(n-k)!},\quad
q=1-p.$$
Закон назван биномиальным, т.к. его можно рассматривать как общий член
разложения бинома Ньютона:
$$(p+q)^n=C_n^n p^n+\cdots+C_n^k p^k q^{n-k}+\cdots+C_n^0 q^k.$$
Таким образом, биномиальный закон описывает вероятность наступления события~$k$
раз в~$n$ независимых испытаниях~\look{binopdf}.

\begin{pict}
\includegraphics[width=.8\textwidth]{pic/binopdf}
\caption{Биномиальное распределение с~$n=100$ и~$p=0.5$ (пунктир). Для
сравнения приведено нормальное распределение с~$\mean{x}=50$ и~$\sigma=7$
(сплошная линия)}
\label{binopdf}
\end{pict}

Если количество испытаний~$n$ велико, можно воспользоваться асимптотической
формулой Лапласа. Однако, формула Лапласа непригодна, если $p\le0.1$. В этих
случаях прибегают к асимптотической формуле Пуассона.
При этом необходимо сделать важное допущение: произведение $np=\lambda$ должно
сохранять постоянное значение (т.е. число появления событий в различных сериях
испытаний остается неизменным).
В этом случае формула Бернулли примет вид:
$$P_n(k)=C_n^k\Bigl(\frac{\lambda}{n}\Bigr)^k\Bigl(1-
\frac{\lambda}{n}\Bigr)^{n-k}.$$
Так как $n$ велико, вместо $P_n(k)$ будем искать $\lim_{n\to\infty}P_n(k)$:
$$P_n(k)\approx \frac{\lambda^k}{k!}\lim_{n\to\infty}
\frac{\overbrace{n(n-1)\cdots[n-(k-1)]}^{\text{$k$ множителей}}}{n^k}
\Bigl(1-\frac{\lambda}{n}\Bigr)^{n-k}=\frac{\lambda^k}{k!}\exp(-\lambda).$$
Эта формула выражает закон\ж распределения
Пуассона\н\index{Распределение!Пуассона} вероятностей массовых и редких
событий~\look{poissonpdf}.

\begin{pict}
\includegraphics[width=.7\textwidth]{pic/poissonpdf}
\caption{Распределение Пуассона с~$\lambda=50$ (пунктир). Для
сравнения приведено нормальное распределение с~$\mean{x}=50$ и~$\sigma=7$
(сплошная линия)}
\label{poissonpdf}
\end{pict}

Важнейшую роль в физике имеет\ж
нормальное\н\index{Распределение!нормальное}~(гауссово) распределение.
Физическая величина подчиняется нормальному распределению, когда она подвержена
влиянию огромного числа случайных помех. Плотность вероятности нормального
распределения имеет вид~\look{normpdf}:
$$
\phi (x) = \frac 1 {\sigma \sqrt {2 \pi}} \exp \left( -\frac {(x -\mean{x})^2}{2
\sigma^2} \right),
$$
Функция распределения Гаусса записывается через\к интеграл
Римана\н\index{Интеграл Римана}:
$$
F (x) = \frac 1{\sigma \sqrt {2 \pi}} \Int_{-\infty}^x \exp \left( -\frac{(t
-\mean{x})^2}{2 \sigma^2} \right)\, dt.
$$
Особенностью нормального распределения является совпадение медианы, моды и
математического ожидания.

\begin{pict}
\includegraphics[width=.8\textwidth]{pic/normpdf}
\caption{Нормальное распределение с~$\mean{x}=5$ и~$\sigma=0.5$. Вертикальными
пунктирными линиями обозначены границы $\mean{x}\pm\sigma$}
\label{normpdf}
\end{pict}

Вероятность того, что нормальная случайная величина с параметрами~$\mean{x}$
и~$\sigma$
попадет в интервал $(\alpha,\beta)$ равна:
$$
P(\alpha < X < \beta) = \Phi\Bigl( \frac {\beta - \mean{x}} {\sigma}\Bigr) -
\Phi\Bigl( \frac {\alpha - \mean{x}} {\sigma}\Bigr),
$$
где $\Phi(x)$~--\ж функция Лапласа\н:
$$
\Phi(x)= \frac 1 {\sqrt {2 \pi}} \Int _{0}^x \exp \Bigl( -\frac {t^2} {2}
\Bigr) dt.
$$

\bf Показательным\н
(экспоненциальным)\index{Распределение!экспоненциальное}~\look{exppdf}
называют распределение вероятностей непрерывной случайной величины~$X$, которое
описывается плотностью
$$f(x)=\begin{cases}
0,& x<0,\\
\lambda\exp(-\lambda x),& x\ge0;
\end{cases}\qquad
F(x)=\begin{cases}
0,& x<0,\\
1-\exp(-\lambda x),& x\ge0,
\end{cases}
$$
где $\lambda$~--- постоянная положительная величина.

\begin{pict}
\includegraphics[width=.7\textwidth]{pic/exppdf}
\caption{Экспоненциальное распределение с~$\lambda=17$ (пунктир). Для
сравнения приведено нормальное распределение с~$\mean{x}=0$ и~$\sigma=7$
(сплошная линия)}
\label{exppdf}
\end{pict}

Для показательного распределения вероятность попадания случайной величины в
интервал~$[a,b]$ равна
$$P(a\le X\le b)=\exp(-\lambda a)-\exp(-\lambda b).$$

Экспоненциальное распределение замечательно тем, что оно описывается лишь одним
параметром~$\lambda$, через который вычисляются и другие характеристики:
$\mean X=\sigma_X=1/\lambda$, $D(X)=1/\lambda^2$.

\subsection{Корреляция и ковариация}
Для выяснения зависимости случайных величин~$X$ и~$Y$ используются такие
функции, как
корреляция и ковариация.\ж Ковариация\н\index{Ковариация} является мерой
линейной зависимости случайных величин и определяется формулой:
$\mathrm{cov}(X,Y)=\mean{(X-\mean{X})(Y-\mean{Y})}$.
Понятно, что ковариация величины с самой собой есть ее дисперсия.\к Ковариация
независимых случайных величин равна нулю\н, обратное неверно.

\bf Коэффициент корреляции\н\index{Коэффициент!корреляции} двух величин задается
формулой
$$
\rho_{X,Y} = \frac{\mathrm{cov}(X,Y)}{\sqrt{D(X) \cdot D(Y)}}.
$$
\it Коэффициент корреляции равен~$\pm1$ тогда и только тогда, когда~$X$ и~$Y$
линейно
зависимы\н. Если они независимы, $\rho_{X,Y}=0$. Промежуточные значения
коэффициента
корреляции не позволяют однозначно судить о зависимости случайных величин, но
позволяет предполагать степень их зависимости.

Для непрерывных величин аналогом коэффициента корреляции является\к
корреляционная
функция\н. Более часто применяют ее разновидность~---\ж автокорреляционную
функцию\н:\index{Функция!автокорреляционная}
$$
\Psi(\tau) = \Int f(t) f(t-\tau)\, dt\equiv
\Int f(t+\tau) f(t)\,dt,
$$
показывающую связь сигнала (функции $f(t)$) с копией самого себя, смещенного на
величину~$\tau$.
Для дискретных случайных величин автокорреляционная функция имеет вид
$$
\Psi(\tau) = \aver{X(t)X(t-\tau)}\equiv\aver{X(t+\tau)X(t)},
$$
где усреднение производится по всем временам~$t$.


Автокорреляционная функция имеет обычно максимум при~$\tau=0$. Имея два сигнала,
представляющих собой исходный и сдвинутый на неизвестную величину~$z$, можно
определить~$z$ из корреляционной функции: эту величину однозначно покажет
положение максимума корреляционной функции~\look{autocorr}.

\begin{pict}
\includegraphics[width=.7\textwidth]{pic/autocorr}
\caption{Участок автокорреляционной функции двух синусоид, сдвинутых друг
относительно друга на~$0.1$~с}
\label{autocorr}
\end{pict}


\subsection{Шум}
В теории сигналов вводят понятие шума.\ж Шум\н\index{Шум}~--- беспорядочные
колебания различной физической природы, отличающиеся сложной временной и
спектральной структурой. В радиоэлектронике под шумом принято понимать любые
нежелательные возмущения, как правило, аддитивно накладывающиеся на полезный
сигнал и искажающие его. 

Шум может носить как случайный, так и систематический характер. Значительно
снизить уровень шума возможно при переходе от аналоговых к цифровым сигналам,
т.к. каждая единица информации при этом может принимать лишь дискретные
значения: промежуточные значения при этом явно будут вызваны шумом.

Наиболее общим видом шума является\ж белый шум\н\index{Белый шум}~--- шум, время
корреляции которого много меньше всех характерных времен физической системы.
Иными словами, это стационарный шум, спектральные составляющие которого
равномерно распределены по всему диапазону задействованных частот.
Математической моделью белого шума является случайный процесс~$\xi(t)$
(важно отметить, что $\mean{\xi(t)}=0$) с автокорреляционной функцией
$$\Psi(t,\tau)=\aver{\xi(t+\tau)\xi(t)}=\sigma^2(t)\delta(\tau).$$
Здесь $\delta(t)$~-- дельта-функция Дирака, $\sigma^2(t)$~-- интенсивность
белого шума.

В случае стационарного процесса $\sigma^2(t)=\const$ и спектр шума является
равномерным:
$$\tilde\Psi(\omega)=\rev{2\pi}\Infint\Psi(\tau)\exp(-i\omega\tau)\,d\tau=
\frac{\sigma^2}{2\pi}.$$

В природе и технике <<чисто>> белый шум (то есть белый шум, имеющий одинаковую
спектральную мощность на всех частотах) не встречается (ввиду того, что\к такой
сигнал имел бы бесконечную мощность\н), однако под категорию белых шумов
попадают любые шумы, спектральная плотность которых одинакова (или слабо
отличается) в рассматриваемом диапазоне частот~\look{SNR}.
Белым шумом является любой шум с фиксированной дисперсией, нулевым
математическим ожиданием и автокорреляционной функцией, имеющей вид
дельта-функции Дирака. Чаще всего белый шум моделируют гауссовским
распределением, т.к. такая модель хорошо подходит для математического описания
многих природных процессов.

\begin{pict}
\includegraphics[width=\textwidth]{pic/SNR}
\caption{Синусоида $y=\sin(x)$ с аддитивным белым шумом с $SNR=10$,
$0$, и~$-10$~дБ (сверху вниз)}
\label{SNR}
\end{pict}

Зашумленность сигнала характеризуют\ж отношением
сигнал/шум\н~(SNR):\index{Отношение сигнал/шум}
$$
\mathrm{SNR} = {P_\mathrm{signal} \over P_\mathrm{noise}} = \left (
{A_\mathrm{signal} \over A_\mathrm{noise} } \right )^2,
$$
где $P$ и~$A$~--- соответственно, средняя мощность и среднеквадратичное значение
амплитуды сигнала и шума. Зачастую~SNR выражают в~децибелах:
$$
\mathrm{SNR (dB)} = 10 \lg \left ( {P_\mathrm{signal} \over P_\mathrm{noise}}
\right )
= 20 \lg \left ( {A_\mathrm{signal} \over A_\mathrm{noise}} \right ).
$$
Причина множителя 10 становится понятной, исходя из приставки <<деци>>, а
множитель~20
возникает вследствие умножения на 2, появляющегося при логарифмировании
амплитуд.

